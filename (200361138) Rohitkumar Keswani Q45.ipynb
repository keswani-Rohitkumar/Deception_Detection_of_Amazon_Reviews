{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"DOC_ID\":  # skip the header\n",
    "                continue\n",
    "            (Id,Text,PRODUCT_Category,Rating,VerifiedReview,Label) = parseReview(line)\n",
    "            rawData.append((Id,Text,PRODUCT_Category,Rating,VerifiedReview,Label))\n",
    "\n",
    "\n",
    "def splitData(percentage):\n",
    "    # A method to split the data between trainData and testData \n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text,PRODUCT_Category,Rating,VerifiedReview,Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text),PRODUCT_Category,Rating,VerifiedReview),Label))\n",
    "    for (_, Text,PRODUCT_Category, Rating, VerifiedReview,Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text),PRODUCT_Category, Rating, VerifiedReview),Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various features which can be considered i experimented with many combinations such as PRODUCT_ID, RATING, PRODUCT_TITLE which gave me the Precision: 0.55, Recall: 0.55, F Score: 0.55, Accuracy: 0.35 on training Data, While on Test Data it gave Precision: 0.32, Recall: 0.35, Fscore: 0.32, Accuracy: 0.35.\n",
    "\n",
    "I also experimented with PRODUCT_ID, PRODUCT_CATEGORY, REVIEW_TITLE which gave me ver low results comparing to above feature selection. The results are Precision: 0.53, Recall: 0.53, F-score: 0.53, Accuracy: 0.35 for training data and Precision: 0.28, Recall: 0.28, F-score: 0.28, Accuracy: 0.28 for testing data. \n",
    "\n",
    "The most efficient result i got was by the following features PRODUCT_Category, Rating, VerifiedReview as in this notebook with Precision: 0.78 , Recall: 0.78, Fscore: 0.77, Accuracy: 0.81 on the training data and \n",
    "Precision:0.81, Recall:0.81, Fscore:0.81, Accuracy:0.81."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    if reviewLine[1] == '__label2__':\n",
    "        reviewLine[1] = realLabel\n",
    "    else:\n",
    "        reviewLine[1] = fakeLabel\n",
    "    return (reviewLine[0], reviewLine[8],reviewLine[4],reviewLine[2],reviewLine[3],reviewLine[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function parseReview with revieLine as an argument it returns Six features consisting of an integer, a string which is the Review_text, a string which is Product category, an integer which is the Rating, next is the Verified_Purchase and lastly is a string which is a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "import re, nltk, string\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "# Input: a string of one review\n",
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    \n",
    "    #tokens=text.split(' ')\n",
    "    \n",
    "    #Remove HTML tags\n",
    "    HTML_TAG_RE = \"<[^>]*\"\n",
    "    text = re.sub(HTML_TAG_RE, ' ', text)\n",
    "    #Remove @ tags\n",
    "    TAG_RE = \"@\\S+\"\n",
    "    text = re.sub(TAG_RE, ' ', text)\n",
    "    #REMOVE WHITE SPACES\n",
    "    text = text.strip()\n",
    "    #REMOVE PUNCTUATION\n",
    "    text = text.translate(text.maketrans('', '', string.punctuation))\n",
    "    #REMOVING WEBSITE LINKS\n",
    "    LINKS_RE = \"https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "    text=re.sub(LINKS_RE, ' ', text)\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    #Stop WORD REMOVAL\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [ i for i in tokens if not i in stop_words]\n",
    "    #PORTER STEMMING\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(j) for j in tokens]\n",
    "    #Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(k) for k in tokens]\n",
    "    #Stemming gave better results than Lemmatization or by using both\n",
    "    bigrams = zip(*[tokens[i:] for i in range (2)])\n",
    "    tokens = [\" \".join(bigram) for bigram in bigrams]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the preprocessing of the function various methods are used for example Removing the Html tags from the text then removing the @ symbols from the text and also removing the white spaces and punctuation. Moreover removing the website links and making the text in lower case is also done in the above function. Following it Stemming and Lemmatization of the text is done.\n",
    "\n",
    "I experimented the preprocessing function by keeping the html tags then by keeping @ tags and again by keeping both html tags and @ tags but there was not a significant change in the result. But while not doing the Stemming of the tokens there was a noticeable change in the results in training data which are as follows\n",
    "With Stemming:-\n",
    "Precision: 0.77, Recall: 0.77, Fscore: 0.77, Accuracy: 0.80\n",
    "Without Stemming:-\n",
    "Precision: 0.80, Recall: 0.80, Fscore: 0.0.80, Accuracy: 0.81\n",
    "While there was not a noticeable change on the test data so i have used stemming and lemmatization both in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDictglobal = {} # A global dictionary of features\n",
    "\n",
    "def toFeatureVector(tokens,PRODUCT_Category,Rating,VerifiedReview):\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    localfeatureDict={}\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            featureDictglobal[t] = featureDictglobal[t]+ 1\n",
    "            localfeatureDict[t] = localfeatureDict[t]+ 1\n",
    "        except KeyError:\n",
    "            featureDictglobal[t] = 1\n",
    "            localfeatureDict[t] = 1\n",
    "    featureDictglobal.update({'PRODUCT_Category':PRODUCT_Category,'Rating':Rating,'VerifiedReview':VerifiedReview})\n",
    "    localfeatureDict.update({'PRODUCT_Category':PRODUCT_Category,'Rating':Rating,'VerifiedReview':VerifiedReview})\n",
    "    return localfeatureDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A global dictionary of features is created as featureDict and featureDictLocal is created for local dictionary.\n",
    "toFeatureVector function with tokens, PRODUCT_Category, Rating, VerifiedReview as argument is used to create a dictionary with features as keys and weights as value in it, Where weight increases as the occurence of the words increases in the text and is added as 1 divided by the length of the tokens in Local feature Dictionary. And finally The global dictionary and local Dictionary are updated with the features which are PRODUCT_Category, Rating, VerifiedReview.\n",
    "\n",
    "I also experimented by adding (1.0/len (tokens) instead of 1 but there was a slight decrease in the preisicon recall and fscore in the training data so i stayed with adding 1 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(penalty='l2',max_iter=2000, loss='hinge',dual=True,random_state=100,\n",
    "                                            verbose=1,C=0.001,class_weight='balanced', fit_intercept=True,\n",
    "                                            intercept_scaling=1,multi_class='ovr'))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above funciton training and validating of our classifier is done but with a few more arguments in the pipeline with maximum iterations of 2000 as with less iterations it was asking to increase the iterations, whereas penalty is assinged to l2 which is better suited for non-sparse case and loss value to hinge because hinge is the Standarad SVM Loss where dual(true because of int in random state) is true as random state is equal to 100 and class_weight is balancd, here the default value of class_weight is None but here we have used Balanced beacuse it automatically adjusts weights inversely proportional to class frequency in the input data and fit intercept is true (to calculate the intercept of the model) while intercept scaling is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "crossValidateActual=[]\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    cv_results = []\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    \n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        TestData = dataset[i:i+foldSize]\n",
    "        TrainData = dataset[:i] + dataset[i+foldSize:]\n",
    "        classifier = trainClassifier(TrainData)\n",
    "        Actual = [x[1] for x in TestData]\n",
    "        PredictedLabels = predictLabels(TestData, classifier)\n",
    "        cv_results = (precision_recall_fscore_support(Actual,PredictedLabels, average='weighted'))\n",
    "    print(\"Accuracy: %f\" % accuracy_score(Actual,PredictedLabels))\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function crossValidate there are two arguments one is the input dataset and second is the fold. In this function we have taken 10 folds. Then the shuffle function is used to reorganize the data and then the foldsize is calculated by dividing the length of the dataset by the number of folds. Then a for loop is used from 0 to the length of the dataset and step argument as a third argument which is the foldsize. This loop is used for TestData and TrainData and trainClassifier function is used on the training data. The crossValidationActual is the actual labels in our test data while the crossValidationPredictLabels predicts the labels from the test data and finally the crossvalidation results are returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above funtions does the prediction of the labels given a classifier where in the first function it takes the review sample whereas in the second fucntion it takes the preprocessed review sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "434109\n",
      "Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Training Classifier...\n",
      "[LibLinear]Accuracy: 0.781548\n",
      "Precision: 0.784388\n",
      "Recall: 0.781548\n",
      "F Score:0.781139\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "trainData = []        # the pre-processed training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the pre-processed test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.8)\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDictglobal), sep='\\n')\n",
    "\n",
    "# QUESTION 3 - Make sure there is a function call here to the\n",
    "# crossValidate function on the training set to get your results\n",
    "validationResults=crossValidate(trainData,10)\n",
    "print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % validationResults[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tha above Main Function the ouput classes are created fakelabel and reallabel. Global lists are created which will be appended by the method which are rawdata trandata and testdata. Now the dataset is parsed into the raw data list. Then the raw Dataset is split into training data and testdata into 80 and 20 percentage respectively. And the data is printed as the number of data in raw data in training data and testing data before and after spliting. And the crossValidate function is called on the training data and the results are printed which are Precision, recall and F Score.\n",
    "\n",
    "It can be clearly observed that the Precision, Recall and Fscore improved drastically on training data by improving the preprocessing function and adding more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'assort realli': 1, 'realli hershey': 1, 'hershey best': 1, 'best littl': 1, 'littl one': 1, 'one alway': 1, 'alway excit': 1, 'excit whenev': 1, 'whenev holiday': 1, 'holiday come': 1, 'PRODUCT_Category': 'Grocery', 'Rating': '5', 'VerifiedReview': 'N'}, 'fake')\n",
      "Training Classifier...\n",
      "[LibLinear]Done training!\n",
      "Precision: 0.814340\n",
      "Recall: 0.808095\n",
      "F Score:0.807137\n",
      "Accuracy: 0.808095\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the tranin data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(testData[0])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier(trainData)  # train the classifier\n",
    "    TTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    Pred = predictLabels(testData, classifier)  #Â classify the test data to get predicted labels\n",
    "    finalScores = precision_recall_fscore_support(TTrue, Pred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])\n",
    "    print(\"Accuracy: %f\" % accuracy_score(TTrue, Pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the accuracy of the classifier is checked by training on all the training data and then testing on the test set.\n",
    "This will work when all the functions are complete and if the functions are complete the first instance of the data is printed and the classifier is trained on the training data, And next is the getting of the ground-truth labels form the data and lastly labels are predicted on the test data and results are printed which are Precision, Recall and F Score.\n",
    "\n",
    "We can clearly deduce that by increasing number of features and improving the data preprocessing function, the Precision, Recall, and F score improved significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions 4 and 5\n",
    "Once you're happy with your functions for Questions 1 to 3, it's advisable you make a copy of this notebook to make a new notebook, and then within it adapt and improve all three functions in the ways asked for in questions 4 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
